# BigData-Predictor

The following system is the basic architecture for the development of a flight predictor assignment. In order to implement the solution, the next systems have been used:

  - Flask server for the development of the web interface. Python language is used.
  - Apache Spark to create the data model and make the predictions. The data model is generated using the algorithm RandomForest. The training process is done PySpark. Onece we have the model any client can generate a request for a new flight and the system will predict the possible delay using the model.
  - Kafka and Zookeeper as a messagging system based on a publish/subscribe system. A topic is generated and all consumer can see the messages related to it.
  - MongoDB as the database to save the predictions made by Spark.

### Development without modifications

For the development of the system it is needed to download the file from GitHub:
> https://github.com/ging/practica_big_data_2019

Afterwards, all the systems are installed executing the scriptInstalacion.sh in the selected directory to develop the services:
```sh
$ ./scriptInstalacion.sh
```
Using this script everything is installed and executed:
 - All the services are downloaded and installed
 - The data model is trained.
 - Kafka and Zookeeper is started up so as to communicate the web service with the prediction job using the topic "flight_delay_classification_request".
 - MongoDB is launched services where the web service look for the prediction, which has been previously saved by Spark Streaming in the database.
 - Spark is started to generate the model and, then, the predictions, that are allocated later in Mongo database.
 - The web service is launched to make a request in the Web Application.

### Development with spark-submit

Spark-submit allow us to execute the prediction job without using IntelliJ. This means, generating a file .jar. This step is also included in the scriptInstalation.sh explained before. The next command is used:
```sh
$ /opt/spark/bin/spark-submit --class es.upm.dit.ging.predictor.MakePrediction --packages org.mongodb.spark:mongo-spark-connector_2.11:2.3.2,org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 /target/scala-2.12/flight_prediction_2.12-0.1.jar
```

were the --class argument is the .scala class to make the prediction and package is the file .jar generated by SBT utils.

### Despliegue mediante docker
The development using Docker has been done following the next steps:
- Creating the Dockerfiles in order to generate the images of the different services Spark, Kafka and Zookeeper and the web service.
- Generating the docker images.
- Creating the docker-compose.yml file so as to connect all the containers.
- Executing the docker-compose.yml

**Create the Dockerfiles**
The dockerfiles can be found in the docker folder of the repository and it has been decided to create the next ones:
- Kafka+Zookeeper Dockerfile because they always are executed together.
- Spark Dockerfile.
- WebService Dockerfile.

With these Dockerfiles we generate the images we are going to use in the docker-compose.yml file. The docker-compose.yml can be executed using the next command in the directory where we have the docker-compose.yml file:
```sh
$ sudo docker-compose up
```

However, to make this task easier it has been developed a scriptEjecucion.sh. So, for the executiong of the system using docker, you must use the next command when you are in the principal folder of the project (/bigdata-predictor):
```sh
$ cd docker
$ ./scriptEjecucion.sh
```

It can be noticed that if we use docker for the implementation we are adding a new layer, so the hostname (localhost) is changed, so we have to indicate the correct hostname and path.

----

[//]: # (These are reference links used in the body of this note and get stripped out when the markdown processor does its job)

   [source]: <https://github.com/ging/practica_big_data_2019>
  



